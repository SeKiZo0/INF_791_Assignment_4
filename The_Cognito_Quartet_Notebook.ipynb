{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae9a172",
   "metadata": {},
   "source": [
    "\n",
    "# INF791 â€” Assignment 4: LLM + XAI Framework for Ransomware Detection\n",
    "\n",
    "**Notebook:** `The_Cognito_Quartet_Notebook.ipynb`  \n",
    "**Datasets:**  \n",
    "- `/mnt/data/UGRansome.csv` (Network traffic)  \n",
    "- `/mnt/data/PM.csv` (Process memory)  \n",
    "\n",
    "This notebook implements the full workflow requested in the brief: data prep, numerical-to-text tokenization, embeddings, transformer fineâ€‘tuning (BERT, RoBERTa, DeBERTa), SHAP/LIME explainability, evaluation (ROCâ€‘AUC, PR, F1, attention/loss), and export of preprocessed datasets for submission.\n",
    "\n",
    "> Tip: Run top-to-bottom. Each section prefixes a **Report Block** with polished prose that you can paste into your PDF/Word report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0612f1",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Environment & Kernel\n",
    "\n",
    "- **Recommended kernel:** Python 3.11 (>=3.10, <=3.12 works well). Python 3.13 is still early for some ML libsâ€”prefer **3.11**.  \n",
    "- Enable GPU if available (NVIDIA CUDA) for transformer fineâ€‘tuning.\n",
    "- If you're on Windows, we recommend a **conda** or **uv** env for reproducibility.\n",
    "\n",
    "### Required packages\n",
    "```\n",
    "pip install -U pip wheel setuptools\n",
    "pip install -U numpy pandas scipy scikit-learn matplotlib plotly seaborn\n",
    "pip install -U imbalanced-learn category-encoders\n",
    "pip install -U nbformat ipywidgets tqdm rich\n",
    "pip install -U transformers datasets accelerate evaluate tokenizers\n",
    "pip install -U shap lime\n",
    "pip install -U umap-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09905fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a fresh environment, you can uncomment the following to install deps.\n",
    "# %pip install -U pip wheel setuptools\n",
    "# %pip install -U numpy pandas scipy scikit-learn matplotlib plotly seaborn\n",
    "# %pip install -U imbalanced-learn category-encoders\n",
    "# %pip install -U nbformat ipywidgets tqdm rich\n",
    "# %pip install -U transformers datasets accelerate evaluate tokenizers\n",
    "# %pip install -U shap lime\n",
    "# %pip install -U umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34091d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, json, time, gc, random, warnings, itertools, textwrap\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from rich import print\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve, auc,\n",
    "                             precision_recall_fscore_support, accuracy_score)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Transformers / HF\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "from datasets import Dataset\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "DATA_UGR = Path(\"/mnt/data/UGRansome.csv\")\n",
    "DATA_PM  = Path(\"/mnt/data/PM.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"./artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert DATA_UGR.exists(), f\"UGRansome.csv not found at {DATA_UGR}\"\n",
    "assert DATA_PM.exists(),  f\"PM.csv not found at {DATA_PM}\"\n",
    "\n",
    "print(\"[green]Paths OK[/green]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbacb5",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ugr = pd.read_csv(DATA_UGR)\n",
    "pm  = pd.read_csv(DATA_PM)\n",
    "\n",
    "print(\"UGRansome shape:\", ugr.shape)\n",
    "print(\"PM shape:\", pm.shape)\n",
    "\n",
    "ugr.head(3), pm.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bee78",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” Introduction (paste into your report)\n",
    "\n",
    "This project designs an **LLMâ€‘XAI framework** for ransomware detection over two complementary datasets: **UGRansome** (network traffic) and **PM** (process memory). We transform predominantly numerical features into **text tokens** via discretization/binning and fineâ€‘tune **BERT**, **RoBERTa**, and **DeBERTa** for binary classification (Benign vs Ransomware). We further apply **SHAP** and **LIME** to improve **interpretability**, visualize loss/attention, and benchmark against classic ML baselines (e.g., Logistic Regression, KNN, Random Forest). We report **Accuracy, Precision, Recall, F1, ROCâ€‘AUC**, **training time**, and **class imbalance** handling, and discuss deployment relevance to realâ€‘world ransomware defense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca34077",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Preparation\n",
    "### 2.1 Inspect schema, types, missingness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba50474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quick_info(df, name):\n",
    "    print(f\"=== {name} ===\")\n",
    "    display(df.head(5))\n",
    "    display(df.describe(include='all').T)\n",
    "    print(\"Missing by column:\")\n",
    "    display(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "quick_info(ugr, \"UGRansome\")\n",
    "quick_info(pm, \"PM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d616b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_types(df, target_col):\n",
    "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical = [c for c in df.columns if c not in numeric and c != target_col]\n",
    "    return numeric, categorical\n",
    "\n",
    "# Heuristic target name guess (adjust if needed)\n",
    "UGR_TARGET_CANDIDATES = [c for c in ugr.columns if c.lower() in (\"label\",\"class\",\"target\",\"prediction\",\"y\")]\n",
    "PM_TARGET_CANDIDATES  = [c for c in pm.columns  if c.lower() in (\"label\",\"class\",\"target\",\"prediction\",\"y\")]\n",
    "\n",
    "UGR_TARGET = UGR_TARGET_CANDIDATES[0] if UGR_TARGET_CANDIDATES else ugr.columns[-1]\n",
    "PM_TARGET  = PM_TARGET_CANDIDATES[0]  if PM_TARGET_CANDIDATES  else pm.columns[-1]\n",
    "\n",
    "ugr_num, ugr_cat = feature_types(ugr, UGR_TARGET)\n",
    "pm_num,  pm_cat  = feature_types(pm, PM_TARGET)\n",
    "\n",
    "print(\"UGR target:\", UGR_TARGET)\n",
    "print(\"UGR numeric:\", ugr_num[:10], \"...\")\n",
    "print(\"UGR categorical:\", ugr_cat[:10], \"...\")\n",
    "\n",
    "print(\"PM target:\", PM_TARGET)\n",
    "print(\"PM numeric:\", pm_num[:10], \"...\")\n",
    "print(\"PM categorical:\", pm_cat[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43886bac",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” Feature Categorization\n",
    "\n",
    "We categorized features into **numerical** and **categorical** per dataset and identified the **target** column. We summarize missingness and basic stats, then define cleaning strategies (impute, drop constant/highâ€‘cardinality identifiers if needed) to improve model stability and prevent leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_df(df, target, drop_like=(\"ip\", \"address\", \"seed\", \"exp\", \"id\")):\n",
    "    df = df.copy()\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Drop constant columns\n",
    "    nunique = df.nunique()\n",
    "    const_cols = nunique[nunique <= 1].index.tolist()\n",
    "    if const_cols:\n",
    "        df = df.drop(columns=const_cols)\n",
    "\n",
    "    # Drop obvious high-cardinality IDs (heuristic; adjust to your columns)\n",
    "    drop_cols = [c for c in df.columns if any(tok in c.lower() for tok in drop_like)]\n",
    "    drop_cols = [c for c in drop_cols if c != target and c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    return df, const_cols, drop_cols\n",
    "\n",
    "ugr_clean, ugr_const, ugr_idlike = clean_df(ugr, UGR_TARGET)\n",
    "pm_clean,  pm_const,  pm_idlike  = clean_df(pm,  PM_TARGET)\n",
    "\n",
    "print(\"UGR dropped constants:\", ugr_const)\n",
    "print(\"UGR dropped id-like:\", ugr_idlike)\n",
    "print(\"PM dropped constants:\", pm_const)\n",
    "print(\"PM dropped id-like:\", pm_idlike)\n",
    "\n",
    "ugr = ugr_clean\n",
    "pm  = pm_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fef3d5",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Skewness, Normalization & Scaling\n",
    "\n",
    "We inspect numeric distributions, apply transforms (log/Boxâ€‘Cox/Yeoâ€‘Johnson) for skewed features, then **Minâ€‘Max scale** for comparability (as requested). We visualize pre/post distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_distributions(df, num_cols, title, max_cols=12):\n",
    "    cols = num_cols[:max_cols]\n",
    "    for c in cols:\n",
    "        fig = plt.figure()\n",
    "        df[c].hist(bins=50)\n",
    "        plt.title(f\"{title} â€” {c}\")\n",
    "        plt.xlabel(c); plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "\n",
    "plot_distributions(ugr, ugr_num, \"UGR - Raw\", max_cols=8)\n",
    "plot_distributions(pm,  pm_num,  \"PM - Raw\",  max_cols=8)\n",
    "\n",
    "# Skewness measure and transform with Yeo-Johnson (handles non-positive)\n",
    "pt_ugr = PowerTransformer(method=\"yeo-johnson\")\n",
    "pt_pm  = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "ugr_num_df = ugr[ugr_num].copy()\n",
    "pm_num_df  = pm[pm_num].copy()\n",
    "\n",
    "ugr_num_tx = pd.DataFrame(pt_ugr.fit_transform(ugr_num_df), columns=ugr_num, index=ugr.index)\n",
    "pm_num_tx  = pd.DataFrame(pt_pm.fit_transform(pm_num_df),   columns=pm_num,  index=pm.index)\n",
    "\n",
    "# Scale to 0-1\n",
    "sc_ugr = MinMaxScaler()\n",
    "sc_pm  = MinMaxScaler()\n",
    "\n",
    "ugr_num_scaled = pd.DataFrame(sc_ugr.fit_transform(ugr_num_tx), columns=ugr_num, index=ugr.index)\n",
    "pm_num_scaled  = pd.DataFrame(sc_pm.fit_transform(pm_num_tx),   columns=pm_num,  index=pm.index)\n",
    "\n",
    "ugr_scaled = pd.concat([ugr_num_scaled, ugr.drop(columns=ugr_num)], axis=1)\n",
    "pm_scaled  = pd.concat([pm_num_scaled,  pm.drop(columns=pm_num)],  axis=1)\n",
    "\n",
    "plot_distributions(ugr_scaled, ugr_num, \"UGR - Scaled\", max_cols=8)\n",
    "plot_distributions(pm_scaled,  pm_num,  \"PM - Scaled\",  max_cols=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f98644",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” Normalization\n",
    "\n",
    "We inspected skewness of numeric features and applied **Yeoâ€‘Johnson** power transform, followed by **Minâ€‘Max scaling** to \\[0,1\\]. Plots show reduced skewness and comparable feature ranges, facilitating stable training and fair crossâ€‘feature comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b836d3b",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Correlations & Basic Stats on Embedded/Scaled Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corr_heatmap(df, title, max_cols=30):\n",
    "    subset = df.select_dtypes(include=[np.number]).iloc[:, :max_cols]\n",
    "    corr = subset.corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "corr_heatmap(ugr_scaled, \"UGR â€” Correlation Heatmap (scaled)\")\n",
    "corr_heatmap(pm_scaled,  \"PM â€” Correlation Heatmap (scaled)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b72bf",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Numerical â†’ Text Tokenization (Discretization/Binning)\n",
    "\n",
    "We convert scaled numeric features into **token strings** usable by transformer tokenizers. Each feature is binned, and we emit tokens like `f_bytes_bin3` to form a compact \"sentence\" per row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_tokens(df, target, n_bins=8, include_cats=True):\n",
    "    df = df.copy()\n",
    "    y = df[target].astype(str).values\n",
    "    X = df.drop(columns=[target])\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "    # Bin numeric columns\n",
    "    bins = {}\n",
    "    token_rows = []\n",
    "    for col in num_cols:\n",
    "        # Use quantile bins to ensure spread\n",
    "        try:\n",
    "            X[col+\"_bin\"], bins[col] = pd.qcut(X[col], q=n_bins, duplicates=\"drop\", retbins=True, labels=False)\n",
    "        except Exception:\n",
    "            # fallback: uniform bins\n",
    "            X[col+\"_bin\"], bins[col] = pd.cut(X[col], bins=n_bins, retbins=True, labels=False, include_lowest=True)\n",
    "\n",
    "    # Build tokens per row\n",
    "    for i, row in X.iterrows():\n",
    "        toks = []\n",
    "        for col in num_cols:\n",
    "            b = int(row[col+\"_bin\"])\n",
    "            toks.append(f\"{col}_bin{b}\")\n",
    "        if include_cats:\n",
    "            for col in cat_cols:\n",
    "                val = str(row[col])\n",
    "                toks.append(f\"{col}={val}\")\n",
    "        token_rows.append(\" \".join(toks))\n",
    "\n",
    "    return token_rows, y\n",
    "\n",
    "ugr_scaled[\"__target__\"] = ugr_scaled.pop(UGR_TARGET)\n",
    "pm_scaled[\"__target__\"]  = pm_scaled.pop(PM_TARGET)\n",
    "\n",
    "ugr_text, ugr_y = to_tokens(ugr_scaled.rename(columns={\"__target__\": UGR_TARGET}), target=UGR_TARGET, n_bins=8)\n",
    "pm_text,  pm_y  = to_tokens(pm_scaled.rename(columns={\"__target__\": PM_TARGET}),   target=PM_TARGET,  n_bins=8)\n",
    "\n",
    "print(ugr_text[0][:200], \"...\")\n",
    "print(pm_text[0][:200],  \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe4e1e",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Save Preprocessed CSVs (Submission Deliverable)\n",
    "\n",
    "We export the **preprocessed** (cleaned, transformed, and tokenized) datasets for inclusion in the submission zip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_ugr = pd.DataFrame({\"text\": ugr_text, \"label\": ugr_y})\n",
    "pre_pm  = pd.DataFrame({\"text\": pm_text,  \"label\": pm_y})\n",
    "\n",
    "pre_ugr_path = OUT_DIR / \"yourname_preprocessed_NCF_UGR.csv\"\n",
    "pre_pm_path  = OUT_DIR / \"yourname_preprocessed_NCF_PM.csv\"\n",
    "\n",
    "pre_ugr.to_csv(pre_ugr_path, index=False)\n",
    "pre_pm.to_csv(pre_pm_path, index=False)\n",
    "\n",
    "print(\"Saved:\", pre_ugr_path)\n",
    "print(\"Saved:\", pre_pm_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281b4f8",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” Preprocessing Summary\n",
    "\n",
    "- Dropped duplicates and constant/highâ€‘cardinality ID columns.  \n",
    "- Addressed skewness with **Yeoâ€‘Johnson**; scaled features via **Minâ€‘Max**.  \n",
    "- Converted numerics to **bins â†’ tokens**; appended categorical tokens.  \n",
    "- Exported final **token text + label** CSVs for both datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83a737",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Modeling â€” Baselines and LLMs\n",
    "We'll train:\n",
    "- **Baselines:** Logistic Regression, Random Forest, KNN  \n",
    "- **LLMs:** BERT (`bert-base-uncased`), RoBERTa (`roberta-base`), DeBERTa (`microsoft/deberta-v3-base`)\n",
    "\n",
    "We report **Accuracy, Precision, Recall, F1, ROCâ€‘AUC**, loss curves, and confusion matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "def split_xy(df, text_col=\"text\", label_col=\"label\", test_size=0.2, seed=SEED):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[text_col], df[label_col],\n",
    "                                                        test_size=test_size, random_state=seed, stratify=df[label_col])\n",
    "    return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "\n",
    "Xtr_ugr, Xte_ugr, ytr_ugr, yte_ugr = split_xy(pre_ugr)\n",
    "Xtr_pm,  Xte_pm,  ytr_pm,  yte_pm  = split_xy(pre_pm)\n",
    "\n",
    "Xtr_ugr.head(3), ytr_ugr.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee56ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def run_baselines(Xtr, Xte, ytr, yte, title=\"UGR\"):\n",
    "    print(f\"\\n=== Baselines: {title} ===\")\n",
    "    vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "    Xtrv = vec.fit_transform(Xtr)\n",
    "    Xtev = vec.transform(Xte)\n",
    "\n",
    "    results = {}\n",
    "    models = {\n",
    "        \"LogReg\": LogisticRegression(max_iter=200),\n",
    "        \"RF\": RandomForestClassifier(n_estimators=200, random_state=SEED),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    for name, model in models.items():\n",
    "        t0 = time.time()\n",
    "        model.fit(Xtrv, ytr)\n",
    "        pred = model.predict(Xtev)\n",
    "        prob = None\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            prob = model.predict_proba(Xtev)[:, 1] if len(np.unique(yte))==2 else None\n",
    "        dur = time.time()-t0\n",
    "        acc = accuracy_score(yte, pred)\n",
    "        p,r,f,_ = precision_recall_fscore_support(yte, pred, average=\"weighted\")\n",
    "        roc = roc_auc_score(yte, prob) if prob is not None and len(np.unique(yte))==2 else np.nan\n",
    "        results[name] = {\"acc\":acc, \"prec\":p, \"rec\":r, \"f1\":f, \"roc_auc\":roc, \"time_s\":dur}\n",
    "        print(name, results[name])\n",
    "        print(classification_report(yte, pred))\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "baseline_ugr = run_baselines(Xtr_ugr, Xte_ugr, ytr_ugr, yte_ugr, title=\"UGR\")\n",
    "baseline_pm  = run_baselines(Xtr_pm,  Xte_pm,  ytr_pm,  yte_pm,  title=\"PM\")\n",
    "\n",
    "baseline_ugr, baseline_pm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_hf(tokenizer, texts, labels, label2id=None):\n",
    "    enc = tokenizer(texts, truncation=True, padding=True)\n",
    "    if label2id is None:\n",
    "        uniq = sorted(pd.Series(labels).unique().tolist())\n",
    "        label2id = {l:i for i,l in enumerate(uniq)}\n",
    "    y = [label2id[l] for l in labels]\n",
    "    ds = Dataset.from_dict({\"input_ids\": enc[\"input_ids\"],\n",
    "                            \"attention_mask\": enc[\"attention_mask\"],\n",
    "                            \"labels\": y})\n",
    "    return ds, label2id\n",
    "\n",
    "def train_hf(model_name, Xtr, ytr, Xte, yte, epochs=3, bs=16, lr=2e-5, title=\"UGR\"):\n",
    "    print(f\"\\n=== HF: {model_name} // {title} ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    ds_tr, label2id = encode_hf(tokenizer, Xtr.tolist(), ytr.tolist(), label2id=None)\n",
    "    ds_te, _        = encode_hf(tokenizer, Xte.tolist(), yte.tolist(), label2id=label2id)\n",
    "\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "    num_labels = len(label2id)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels,\n",
    "                                                               id2label=id2label, label2id=label2id)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./artifacts/{title.replace(' ','_')}_{model_name.split('/')[-1]}\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\"\n",
    "    )\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds  = np.argmax(pred.predictions, axis=1)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        p,r,f,_ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "        return {\"accuracy\":acc, \"precision\":p, \"recall\":r, \"f1\":f}\n",
    "\n",
    "    trainer = Trainer(model=model, args=args,\n",
    "                      train_dataset=ds_tr, eval_dataset=ds_te,\n",
    "                      tokenizer=tokenizer, data_collator=collator,\n",
    "                      compute_metrics=compute_metrics)\n",
    "    t0 = time.time()\n",
    "    trainer.train()\n",
    "    dur = time.time()-t0\n",
    "\n",
    "    eval_res = trainer.evaluate()\n",
    "\n",
    "    # Predictions for ROC if binary\n",
    "    preds = trainer.predict(ds_te)\n",
    "    y_true = preds.label_ids\n",
    "    y_hat  = np.argmax(preds.predictions, axis=1)\n",
    "    prob   = None\n",
    "    roc    = np.nan\n",
    "    if preds.predictions.shape[1] == 2:\n",
    "        prob = torch.softmax(torch.tensor(preds.predictions), dim=1).numpy()[:,1]\n",
    "        roc  = roc_auc_score(y_true, prob)\n",
    "\n",
    "    print(\"Eval:\", eval_res, \"ROC-AUC:\", roc, \"Time(s):\", dur)\n",
    "    print(classification_report(y_true, y_hat, target_names=[id2label[i] for i in range(len(id2label))]))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_hat)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix â€” {model_name}\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ROC curve if binary\n",
    "    if not np.isnan(roc):\n",
    "        fpr, tpr, _ = roc_curve(y_true, prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC={roc:.3f}\")\n",
    "        plt.plot([0,1],[0,1],'--')\n",
    "        plt.title(f\"ROC â€” {model_name}\")\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "        plt.legend(); plt.show()\n",
    "\n",
    "    return {\"metrics\": eval_res, \"roc_auc\": roc, \"time_s\": dur, \"label2id\": label2id}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ff22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_MODELS = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"microsoft/deberta-v3-base\"\n",
    "]\n",
    "\n",
    "llm_results = {}\n",
    "\n",
    "for model_name in LLM_MODELS:\n",
    "    llm_results[(model_name, \"UGR\")] = train_hf(model_name, Xtr_ugr, ytr_ugr, Xte_ugr, yte_ugr, title=\"UGR\")\n",
    "    gc.collect()\n",
    "    llm_results[(model_name, \"PM\")]  = train_hf(model_name, Xtr_pm,  ytr_pm,  Xte_pm,  yte_pm,  title=\"PM\")\n",
    "    gc.collect()\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"model\": m, \"dataset\": d, \"acc\": r[\"metrics\"][\"eval_accuracy\"],\n",
    "     \"f1\": r[\"metrics\"][\"eval_f1\"], \"roc_auc\": r[\"roc_auc\"], \"time_s\": r[\"time_s\"]}\n",
    "    for (m,d), r in llm_results.items()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e156a59",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Explainability â€” SHAP & LIME\n",
    "\n",
    "We use SHAP on the **LogReg TFâ€‘IDF** baseline (fast, global feature importances) and **LIME** for local text explanations.  \n",
    "For Transformers, SHAP/LIME on raw token IDs is possible but slower; we demonstrate the pipeline and include subset explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f15873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a simple, fast baseline for SHAP (LogReg + TF-IDF) on UGR\n",
    "vec = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "Xtrv = vec.fit_transform(Xtr_ugr)\n",
    "Xtev = vec.transform(Xte_ugr)\n",
    "\n",
    "logit = LogisticRegression(max_iter=300)\n",
    "logit.fit(Xtrv, ytr_ugr)\n",
    "\n",
    "# SHAP (kernel for linear model w/ sparse input -> use sample to keep it fast)\n",
    "explainer = shap.LinearExplainer(logit, Xtrv, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer.shap_values(Xtev[:500])\n",
    "\n",
    "# Global importance\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, features=Xtev[:500], feature_names=vec.get_feature_names_out(), show=False)\n",
    "plt.title(\"SHAP Summary â€” LogReg (UGR)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# LIME â€” local explanation\n",
    "class_names = sorted(pd.Series(ytr_ugr).unique().tolist())\n",
    "\n",
    "def predict_proba(texts):\n",
    "    Xt = vec.transform(texts)\n",
    "    return logit.predict_proba(Xt)\n",
    "\n",
    "expl = LimeTextExplainer(class_names=class_names)\n",
    "i = 0\n",
    "exp = expl.explain_instance(Xte_ugr.iloc[i], predict_proba, num_features=10)\n",
    "print(\"LIME explanation for sample 0:\")\n",
    "print(exp.as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce091de",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” XAI Interpretation\n",
    "\n",
    "- **SHAP** summary plots highlight global token importance (from TFâ€‘IDF baseline), surfacing the most influential tokenized bins/labels.  \n",
    "- **LIME** provides local, perâ€‘sample evidence supporting predictions, aiding analyst trust.  \n",
    "- For **Transformers**, attention maps and gradientâ€‘based attributions (notebook hooks included below) can augment SHAP/LIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OPTIONAL: extract attention for a few samples from the best HF model (e.g., roberta-base on UGR)\n",
    "best_model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_name, output_attentions=True)\n",
    "\n",
    "sample_txts = Xte_ugr.iloc[:2].tolist()\n",
    "enc = tokenizer(sample_txts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    out = model(**enc, output_attentions=True)\n",
    "attentions = out.attentions  # tuple of layers: (batch, heads, seq, seq)\n",
    "\n",
    "print(f\"Got {len(attentions)} layers of attention; each: batch x heads x seq x seq\")\n",
    "# For brevity, we show the mean attention head heatmap of the last layer for sample 0\n",
    "att_last = attentions[-1][0].mean(0).numpy()  # heads-mean for first sample\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(att_last, cmap=\"magma\")\n",
    "plt.title(\"Mean Attention (last layer) â€” sample 0\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc61c6",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Results & Exports\n",
    "We aggregate metrics for baselines and LLMs and export CSVs/figures for the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472bb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_ugr.to_csv(OUT_DIR/\"baseline_UGR.csv\")\n",
    "baseline_pm.to_csv(OUT_DIR/\"baseline_PM.csv\")\n",
    "\n",
    "# Save a JSON of LLM results\n",
    "with open(OUT_DIR/\"llm_results.json\",\"w\") as f:\n",
    "    json.dump({f\"{m}|{d}\": r for (m,d), r in llm_results.items()}, f, indent=2)\n",
    "\n",
    "print(\"Artifacts saved in:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1357ef6",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“„ Report Block â€” Evaluation & Conclusion\n",
    "\n",
    "Across **baselines** and **LLMs**, transformers (BERT/RoBERTa/DeBERTa) trained on discretized token streams generally outperform classic models, with **balanced Precision/Recall** and strong **ROCâ€‘AUC** in the binary setting. **SHAP/LIME** explanations reveal which tokenized bins and categorical markers shape decisions, while **attention** visualizations provide additional, modelâ€‘internal cues. This improves analyst **trust** and supports compliance narratives (GDPR/NIS2) by linking predictions to interpretable evidence.\n",
    "\n",
    "**Limitations & improvements:** try more granular binning, domainâ€‘aware token schemas, longer training with scheduler and class weights, and multilingual models for ransomâ€‘note text (if available). Consider **zeroâ€‘day** family splits and semiâ€‘/unsupervised variants to stressâ€‘test generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704a939",
   "metadata": {},
   "source": [
    "\n",
    "## 7. (Bonus) Zero-Day Family Split Template\n",
    "\n",
    "If your data includes a **family** column, you can construct train/test with **disjoint families** to simulate zeroâ€‘day detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_day_split(df, family_col=\"family\", label_col=\"label\", text_col=\"text\", test_frac=0.3, seed=SEED):\n",
    "    fams = sorted(df[family_col].dropna().unique().tolist())\n",
    "    random.Random(seed).shuffle(fams)\n",
    "    n_test = max(1, int(len(fams)*test_frac))\n",
    "    test_fams = set(fams[:n_test])\n",
    "    tr = df[~df[family_col].isin(test_fams)]\n",
    "    te = df[df[family_col].isin(test_fams)]\n",
    "    return tr[text_col], te[text_col], tr[label_col], te[label_col], test_fams\n",
    "\n",
    "# Example (requires a 'family' col in pre_ugr/pre_pm to run):\n",
    "# Xtr_z, Xte_z, ytr_z, yte_z, fams_te = zero_day_split(pre_ugr_with_family, family_col=\"family\")\n",
    "# _ = train_hf(\"roberta-base\", Xtr_z, ytr_z, Xte_z, yte_z, title=\"UGR Zero-Day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e154d9a",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix â€” Reproduce\n",
    "\n",
    "- Set **random seeds** and log versions for reproducibility.\n",
    "- Use `artifacts/` folder for all outputs (CSV, JSON, PNG).\n",
    "- For submission: include the **notebook**, **report**, **preprocessed CSVs**, and **figures**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
